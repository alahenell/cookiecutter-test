{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0ac7f0575b8369",
   "metadata": {},
   "source": "# Benchmark Simulation for Two-Layer (unknown)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===== Benchmark Script for Two-Layer Causal Simulation =====\n",
    "# Goal: Evaluate layer-wise causal feature identification\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from scipy.stats import norm\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import ray\n",
    "from ray import tune\n",
    "import gc\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from velorama.train import *\n",
    "from velorama.utils import *\n",
    "from CauTrigger.utils import set_seed\n",
    "from CauTrigger.model import CauTrigger2L, CauTrigger1L\n",
    "from CauTrigger.dataloaders import generate_two_layer_synthetic_data\n",
    "\n",
    "sys.path.append(\"../\")  # 加入 CauTrigger 主目录\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"matplotlib.font_manager\").disabled = True\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Arial\"]\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27de3cc33f0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CauTrigger(adata, output_dir=None, mode=\"SHAP\", topk=20, is_log1p=False, full_input=False):\n",
    "    \"\"\"\n",
    "    Run CauTrigger in two steps:\n",
    "    Step 1: Use CauTrigger1L on layer1 (closer to Y) to get top-k causal genes.\n",
    "    Step 2: Use those genes as X_down in CauTrigger2L to score layer2 (upstream).\n",
    "    Return:\n",
    "        dict {\n",
    "            \"layer1\": DataFrame with scores (index = layer1 gene names),\n",
    "            \"layer2\": DataFrame with scores (index = layer2 gene names)\n",
    "        }\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    if is_log1p:\n",
    "        print(\"CauTrigger log1p normalization applied to adata.X, adata.obsm['layer1'], and adata.obsm['layer2']\")\n",
    "        adata.X = np.log1p(adata.X)\n",
    "        adata.obsm[\"layer1\"] = np.log1p(adata.obsm[\"layer1\"])\n",
    "        adata.obsm[\"layer2\"] = np.log1p(adata.obsm[\"layer2\"])\n",
    "        adata._log1p_applied = True\n",
    "        print(\"[INFO] log1p transformation applied to adata.obsm['layer1'] and ['layer2'].\")\n",
    "\n",
    "    # === Step 1: layer1 → downstream (closer to Y) ===\n",
    "    layer1_vars = adata.var_names[adata.var[\"layer\"] == \"layer1\"]\n",
    "    adata_layer1 = AnnData(X=adata.obsm[\"layer1\"], obs=adata.obs.copy(), var=adata.var.loc[layer1_vars].copy())\n",
    "\n",
    "    model_1L = CauTrigger1L(\n",
    "        adata_layer1,\n",
    "        n_latent=10,\n",
    "        n_hidden=128,\n",
    "        n_layers_encoder=0,\n",
    "        n_layers_decoder=0,\n",
    "        n_layers_dpd=0,\n",
    "        dropout_rate_encoder=0.0,\n",
    "        dropout_rate_decoder=0.0,\n",
    "        dropout_rate_dpd=0.0,\n",
    "        use_batch_norm=\"none\",\n",
    "        use_batch_norm_dpd=True,\n",
    "        decoder_linear=False,\n",
    "        dpd_linear=True,\n",
    "        init_weight=None,\n",
    "        init_thresh=0.4,\n",
    "        attention=False,\n",
    "        att_mean=False,\n",
    "    )\n",
    "    model_1L.train(max_epochs=200, stage_training=True, weight_scheme=\"sim\")\n",
    "    df_layer1, _ = model_1L.get_up_feature_weights(method=mode, normalize=False, sort_by_weight=True)\n",
    "    print(\"df_layer1\", df_layer1.head(20))\n",
    "\n",
    "    # === Step 2: layer2 → upstream, use layer1 top-k as X_down ===\n",
    "    layer2_vars = adata.var_names[adata.var[\"layer\"] == \"layer2\"]\n",
    "    df_layer1 = df_layer1.loc[layer1_vars]  # ensure order matches obsm[\"layer1\"]\n",
    "    topk_indices = df_layer1[\"weight\"].values.argsort()[-topk:]\n",
    "    X_down = adata.obsm[\"layer1\"] if full_input else adata.obsm[\"layer1\"][:, topk_indices]\n",
    "\n",
    "    adata_layer2 = AnnData(\n",
    "        X=adata.obsm[\"layer2\"], obs=adata.obs.copy(), var=adata.var.loc[layer2_vars].copy(), obsm={\"X_down\": X_down}\n",
    "    )\n",
    "\n",
    "    model_2L = CauTrigger2L(\n",
    "        adata_layer2,\n",
    "        n_latent=10,\n",
    "        n_hidden=128,\n",
    "        n_layers_encoder=0,\n",
    "        n_layers_decoder=0,\n",
    "        n_layers_dpd=0,\n",
    "        dropout_rate_encoder=0.0,\n",
    "        dropout_rate_decoder=0.0,\n",
    "        dropout_rate_dpd=0.0,\n",
    "        use_batch_norm=\"none\",\n",
    "        use_batch_norm_dpd=True,\n",
    "        decoder_linear=False,\n",
    "        dpd_linear=True,\n",
    "        init_weight=None,\n",
    "        init_thresh=0.4,\n",
    "        attention=False,\n",
    "        att_mean=False,\n",
    "    )\n",
    "    model_2L.train(max_epochs=200, stage_training=True, weight_scheme=\"sim\")\n",
    "    df_layer2, _ = model_2L.get_up_feature_weights(method=mode, normalize=False, sort_by_weight=True)\n",
    "    print(\"df_layer2\", df_layer2.head(10))\n",
    "    df_layer2 = df_layer2.loc[layer2_vars]\n",
    "\n",
    "    # Set correct index for both outputs\n",
    "    assert df_layer1.index.equals(layer1_vars)\n",
    "    assert df_layer2.index.equals(layer2_vars)\n",
    "\n",
    "    return {\n",
    "        \"layer1\": df_layer1,\n",
    "        \"layer2\": df_layer2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a88c095c105a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PC(adata, output_dir):\n",
    "    def gauss_ci_test(suff_stat, i, j, K):\n",
    "        corr_matrix = suff_stat[\"C\"]\n",
    "        n_samples = suff_stat[\"n\"]\n",
    "\n",
    "        if len(K) == 0:\n",
    "            r = corr_matrix[i, j]\n",
    "        elif len(K) == 1:\n",
    "            k = K[0]\n",
    "            r = (corr_matrix[i, j] - corr_matrix[i, k] * corr_matrix[j, k]) / math.sqrt(\n",
    "                (1 - corr_matrix[i, k] ** 2) * (1 - corr_matrix[j, k] ** 2)\n",
    "            )\n",
    "        else:\n",
    "            sub_corr = corr_matrix[np.ix_([i, j] + K, [i, j] + K)]\n",
    "            precision_matrix = np.linalg.pinv(sub_corr)\n",
    "            r = (-1 * precision_matrix[0, 1]) / math.sqrt(abs(precision_matrix[0, 0] * precision_matrix[1, 1]))\n",
    "\n",
    "        r = max(min(r, 0.99999), -0.99999)\n",
    "        z = 0.5 * math.log1p((2 * r) / (1 - r))\n",
    "        z_standard = z * math.sqrt(n_samples - len(K) - 3)\n",
    "        p_value = 2 * (1 - norm.cdf(abs(z_standard)))\n",
    "\n",
    "        return p_value\n",
    "\n",
    "    def get_neighbors(G, x, exclude_y):\n",
    "        return [i for i, connected in enumerate(G[x]) if connected and i != exclude_y]\n",
    "\n",
    "    def skeleton(suff_stat, alpha):\n",
    "        p_value_mat = np.zeros_like(suff_stat[\"C\"])\n",
    "        n_nodes = suff_stat[\"C\"].shape[0]\n",
    "        O = [[[] for _ in range(n_nodes)] for _ in range(n_nodes)]\n",
    "        G = [[i != j for i in range(n_nodes)] for j in range(n_nodes)]\n",
    "        pairs = [(i, j) for i in range(n_nodes) for j in range(i + 1, n_nodes)]\n",
    "\n",
    "        done = False\n",
    "        l = 0\n",
    "\n",
    "        while not done and any(any(row) for row in G):\n",
    "            done = True\n",
    "\n",
    "            for x, y in pairs:\n",
    "                if G[x][y]:\n",
    "                    neighbors = get_neighbors(G, x, y)\n",
    "                    if len(neighbors) >= l:\n",
    "                        done = False\n",
    "                        for K in combinations(neighbors, l):\n",
    "                            p_value = gauss_ci_test(suff_stat, x, y, list(K))\n",
    "                            if p_value > p_value_mat[x][y]:\n",
    "                                p_value_mat[x][y] = p_value_mat[y][x] = p_value\n",
    "                            if p_value >= alpha:\n",
    "                                G[x][y] = G[y][x] = False\n",
    "                                O[x][y] = O[y][x] = list(K)\n",
    "                                break\n",
    "            l += 1\n",
    "\n",
    "        return np.asarray(G, dtype=int), O, p_value_mat\n",
    "\n",
    "    def extend_cpdag(G, O):\n",
    "        n_nodes = G.shape[0]\n",
    "\n",
    "        def rule1(g):\n",
    "            pairs = [(i, j) for i in range(n_nodes) for j in range(n_nodes) if g[i][j] == 1 and g[j][i] == 0]\n",
    "            for i, j in pairs:\n",
    "                all_k = [\n",
    "                    k for k in range(n_nodes) if (g[j][k] == 1 and g[k][j] == 1) and (g[i][k] == 0 and g[k][i] == 0)\n",
    "                ]\n",
    "                for k in all_k:\n",
    "                    g[j][k] = 1\n",
    "                    g[k][j] = 0\n",
    "            return g\n",
    "\n",
    "        def rule2(g):\n",
    "            pairs = [(i, j) for i in range(n_nodes) for j in range(n_nodes) if g[i][j] == 1 and g[j][i] == 1]\n",
    "            for i, j in pairs:\n",
    "                all_k = [\n",
    "                    k for k in range(n_nodes) if (g[i][k] == 1 and g[k][i] == 0) and (g[k][j] == 1 and g[j][k] == 0)\n",
    "                ]\n",
    "                if len(all_k) > 0:\n",
    "                    g[i][j] = 1\n",
    "                    g[j][i] = 0\n",
    "            return g\n",
    "\n",
    "        def rule3(g):\n",
    "            pairs = [(i, j) for i in range(n_nodes) for j in range(n_nodes) if g[i][j] == 1 and g[j][i] == 1]\n",
    "            for i, j in pairs:\n",
    "                all_k = [\n",
    "                    k for k in range(n_nodes) if (g[i][k] == 1 and g[k][i] == 1) and (g[k][j] == 1 and g[j][k] == 0)\n",
    "                ]\n",
    "                if len(all_k) >= 2:\n",
    "                    for k1, k2 in combinations(all_k, 2):\n",
    "                        if g[k1][k2] == 0 and g[k2][k1] == 0:\n",
    "                            g[i][j] = 1\n",
    "                            g[j][i] = 0\n",
    "                            break\n",
    "            return g\n",
    "\n",
    "        pairs = [(i, j) for i in range(n_nodes) for j in range(n_nodes) if G[i][j] == 1]\n",
    "        for x, y in sorted(pairs, key=lambda x: (x[1], x[0])):\n",
    "            all_z = [z for z in range(n_nodes) if G[y][z] == 1 and z != x]\n",
    "            for z in all_z:\n",
    "                if G[x][z] == 0 and y not in O[x][z]:\n",
    "                    G[x][y] = G[z][y] = 1\n",
    "                    G[y][x] = G[y][z] = 0\n",
    "\n",
    "        old_G = np.zeros((n_nodes, n_nodes))\n",
    "        while not np.array_equal(old_G, G):\n",
    "            old_G = G.copy()\n",
    "            G = rule1(G)\n",
    "            G = rule2(G)\n",
    "            G = rule3(G)\n",
    "\n",
    "        return np.array(G)\n",
    "\n",
    "    def pc(suff_stat, alpha=0.5, verbose=False):\n",
    "        G, O, pvm = skeleton(suff_stat, alpha)\n",
    "        cpdag = extend_cpdag(G, O)\n",
    "        if verbose:\n",
    "            print(cpdag)\n",
    "        return cpdag, pvm\n",
    "\n",
    "    alpha = 0.05\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:\n",
    "        X = np.log1p(X)\n",
    "    y = adata.obs[\"labels\"].values\n",
    "    data = pd.DataFrame(np.column_stack((X, y)))\n",
    "    cpdag, pvm = pc(suff_stat={\"C\": data.corr().values, \"n\": data.shape[0]}, alpha=alpha)\n",
    "    pv = pvm[:-1, -1]\n",
    "    arr = np.array(1 - pv).reshape(1, -1)  # 需要转换成 2D\n",
    "    normalized_arr = sk_normalize(arr, norm=\"l1\", axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_VAE(adata, output_dir):\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:  # Rough check for count data\n",
    "        X = np.log1p(X)  # Apply log1p for count data\n",
    "    y = adata.obs[\"labels\"].values\n",
    "    n_features = X.shape[1]\n",
    "    features = torch.tensor(X, dtype=torch.float32)\n",
    "    labels = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "    dataset = TensorDataset(features, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    n_hidden = 5\n",
    "    n_latent = 5\n",
    "\n",
    "    class VAE(nn.Module):\n",
    "        def __init__(self, num_features):\n",
    "            super().__init__()\n",
    "\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(num_features, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, 2 * n_latent),\n",
    "            )\n",
    "\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(n_latent, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, num_features),\n",
    "                # nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "            self.DPD = nn.Sequential(\n",
    "                nn.Linear(n_latent, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, 1),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "        def reparameterize(self, mu, logvar):\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps * std + mu\n",
    "\n",
    "        def forward(self, x):\n",
    "            mu_logvar = self.encoder(x)\n",
    "            mu = mu_logvar[:, :n_latent]\n",
    "            logvar = mu_logvar[:, n_latent:]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            y = self.DPD(z)\n",
    "            reconstructed = self.decoder(z)\n",
    "            return reconstructed, y, mu, logvar\n",
    "\n",
    "    model = VAE(n_features)\n",
    "    recon_criterion = nn.MSELoss()\n",
    "    dpd_criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    losses = []\n",
    "    re_losses = []\n",
    "    kl_losses = []\n",
    "    dpd_losses = []\n",
    "    for epoch in range(200):\n",
    "        for data, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, y_dpd, mu, logvar = model(data)\n",
    "            # reconstructed loss\n",
    "            re_loss = recon_criterion(recon_batch, data)\n",
    "            re_losses.append(re_loss.item())\n",
    "\n",
    "            # kl loss\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / data.shape[0]\n",
    "            kl_losses.append(kl_loss.item())\n",
    "\n",
    "            # dpd loss\n",
    "            dpd_loss = dpd_criterion(y_dpd, targets)\n",
    "            dpd_losses.append(dpd_loss.item())\n",
    "\n",
    "            # total loss\n",
    "            if epoch <= 100:\n",
    "                loss = re_loss + kl_loss * 0.1 + dpd_loss * 0.1\n",
    "            else:\n",
    "                loss = re_loss + kl_loss * 0.1 + dpd_loss * 0.1\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    features.requires_grad = True\n",
    "    _, y_prob, _, _ = model(features)\n",
    "    loss = dpd_criterion(y_prob, labels)\n",
    "    loss.backward()\n",
    "    grads = features.grad.abs()\n",
    "    grad_features_importance = grads.mean(dim=0)\n",
    "    grad_df = grad_features_importance.detach().numpy()\n",
    "    arr = np.array(grad_df).reshape(1, -1)  # 需要转换成 2D\n",
    "    normalized_arr = sk_normalize(arr, norm=\"l1\", axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_RF(adata, output_dir):\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:\n",
    "        X = np.log1p(X)\n",
    "    y = adata.obs[\"labels\"].values\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X, y)\n",
    "    arr = np.array(rf.feature_importances_.flatten()).reshape(1, -1)\n",
    "    normalized_arr = sk_normalize(arr, norm=\"l1\", axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_SVM(adata, output_dir):\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:\n",
    "        X = np.log1p(X)\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    y = adata.obs[\"labels\"].values\n",
    "    svm = SVC(kernel=\"linear\")\n",
    "    svm.fit(X, y)\n",
    "    arr = np.array(svm.coef_.flatten()).reshape(1, -1)\n",
    "    normalized_arr = sk_normalize(arr, norm=\"l1\", axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_MI(adata, output_dir):\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:\n",
    "        X = np.log1p(X)\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    y = adata.obs[\"labels\"].values\n",
    "    arr = np.array(mutual_info_classif(X, y).flatten()).reshape(1, -1)\n",
    "    normalized_arr = sk_normalize(arr, norm=\"l1\", axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_VELORAMA(adata, output_dir):\n",
    "    target_genes = adata.var_names.tolist()\n",
    "    reg_genes = adata.var_names.tolist()\n",
    "\n",
    "    X_orig = adata.X.copy()\n",
    "    if np.issubdtype(X_orig.dtype, np.integer) or X_orig.max() > 100:\n",
    "        X_orig = np.log1p(X_orig)\n",
    "    std = X_orig.std(0)\n",
    "    std[std == 0] = 1\n",
    "    X = torch.FloatTensor(X_orig - X_orig.mean(0)) / std\n",
    "    X = X.to(torch.float32)\n",
    "\n",
    "    Y_orig = adata.X.copy()\n",
    "    std = Y_orig.std(0)\n",
    "    std[std == 0] = 1\n",
    "    Y = torch.FloatTensor(Y_orig - Y_orig.mean(0)) / std\n",
    "    Y = Y.to(torch.float32)\n",
    "\n",
    "    adata.uns[\"iroot\"] = 0\n",
    "    sc.pp.scale(adata)\n",
    "\n",
    "    reg_target = 0\n",
    "    dynamics = \"pseudotime\"\n",
    "    ptloc = \"pseudotime\"\n",
    "    proba = 1\n",
    "    n_neighbors = 30\n",
    "    velo_mode = \"stochastic\"\n",
    "    time_series = 0\n",
    "    n_comps = 20\n",
    "    lag = 5\n",
    "    name = \"velorama_run\"\n",
    "    seed = 42\n",
    "    hidden = 32\n",
    "    penalty = \"H\"\n",
    "    save_dir = output_dir\n",
    "    lam_start = -2\n",
    "    lam_end = 1\n",
    "    num_lambdas = 19\n",
    "\n",
    "    A = construct_dag(\n",
    "        adata,\n",
    "        dynamics=dynamics,\n",
    "        ptloc=ptloc,\n",
    "        proba=proba,\n",
    "        n_neighbors=n_neighbors,\n",
    "        velo_mode=velo_mode,\n",
    "        use_time=time_series,\n",
    "        n_comps=n_comps,\n",
    "    )\n",
    "    A = torch.FloatTensor(A)\n",
    "    AX = calculate_diffusion_lags(A, X, lag)\n",
    "    AY = None\n",
    "\n",
    "    dir_name = f\"{name}.seed{seed}.h{hidden}.{penalty}.lag{lag}.{dynamics}\"\n",
    "\n",
    "    if not os.path.exists(os.path.join(save_dir, dir_name)):\n",
    "        os.makedirs(os.path.join(save_dir, dir_name), exist_ok=True)\n",
    "\n",
    "    ray.init(object_store_memory=6 * 1024**3, ignore_reinit_error=True)  # 可以调大\n",
    "\n",
    "    lam_list = np.logspace(lam_start, lam_end, num=num_lambdas).tolist()\n",
    "\n",
    "    config = {\n",
    "        \"name\": name,\n",
    "        \"AX\": AX,\n",
    "        \"AY\": AY,\n",
    "        \"Y\": Y,\n",
    "        \"seed\": seed,\n",
    "        \"lr\": 0.01,\n",
    "        \"lam\": tune.grid_search(lam_list),\n",
    "        \"lam_ridge\": 0.0,\n",
    "        \"penalty\": penalty,\n",
    "        \"lag\": lag,\n",
    "        \"hidden\": [hidden],\n",
    "        \"max_iter\": 200,\n",
    "        \"device\": \"cpu\",\n",
    "        \"lookback\": 5,\n",
    "        \"check_every\": 10,\n",
    "        \"verbose\": True,\n",
    "        \"dynamics\": dynamics,\n",
    "        \"results_dir\": save_dir,\n",
    "        \"dir_name\": dir_name,\n",
    "        \"reg_target\": reg_target,\n",
    "    }\n",
    "    resources_per_trial = {\"cpu\": 1, \"gpu\": 0, \"memory\": 1 * 1024**3}\n",
    "    analysis = tune.run(train_model, resources_per_trial=resources_per_trial, config=config, storage_path=save_dir)\n",
    "\n",
    "    target_dir = os.path.join(save_dir, dir_name)\n",
    "    base_dir = save_dir\n",
    "    move_files(base_dir, target_dir)\n",
    "\n",
    "    lam_list = [np.round(lam, 4) for lam in lam_list]\n",
    "    all_lags = load_gc_interactions(\n",
    "        name,\n",
    "        save_dir,\n",
    "        lam_list,\n",
    "        hidden_dim=hidden,\n",
    "        lag=lag,\n",
    "        penalty=penalty,\n",
    "        dynamics=dynamics,\n",
    "        seed=seed,\n",
    "        ignore_lag=False,\n",
    "    )\n",
    "\n",
    "    gc_mat = estimate_interactions(all_lags, lag=lag)  # tg_count x tf_count\n",
    "    gc_df = pd.DataFrame(gc_mat.cpu().data.numpy(), index=target_genes, columns=reg_genes)\n",
    "    ray.shutdown()\n",
    "    return gc_df.mean(axis=0).values\n",
    "\n",
    "\n",
    "def run_SCRIBE(adata, output_dir):\n",
    "    from Scribe.read_export import load_anndata\n",
    "\n",
    "    adata = adata.copy()\n",
    "    sc.pp.log1p(adata)\n",
    "    adata.uns[\"iroot\"] = 0\n",
    "    sc.pp.scale(adata)\n",
    "    sc.pp.neighbors(adata)\n",
    "    sc.tl.dpt(adata)\n",
    "    adata.obs[\"dpt_groups\"] = [\n",
    "        \"0\" if i < adata.obs[\"dpt_pseudotime\"].median() else \"1\" for i in adata.obs[\"dpt_pseudotime\"]\n",
    "    ]\n",
    "\n",
    "    model = load_anndata(adata)\n",
    "    model.rdi(\n",
    "        delays=[1, 2, 3], number_of_processes=1, uniformization=False, differential_mode=False\n",
    "    )  # dict_keys([1, 2, 3, 'MAX'])\n",
    "\n",
    "    edges = []\n",
    "    values = []\n",
    "    for id1 in adata.var_names:\n",
    "        for id2 in adata.var_names:\n",
    "            if id1 == id2:\n",
    "                continue\n",
    "            edges.append(id1.lower() + \"\\t\" + id2.lower())\n",
    "            values.append(model.rdi_results[\"MAX\"].loc[id1, id2])\n",
    "\n",
    "    edges_values = [[edges[i], values[i]] for i in range(len(edges))]\n",
    "    df = pd.DataFrame(edges_values, columns=[\"Edge\", \"Value\"])\n",
    "    df[[\"Source\", \"Target\"]] = df[\"Edge\"].str.split(\"\\t\", expand=True)\n",
    "    df_sorted = df[[\"Source\", \"Target\", \"Value\"]].sort_values(by=\"Value\", ascending=False)\n",
    "    df_mean = df_sorted.groupby(\"Source\")[\"Value\"].mean().reset_index()\n",
    "    df_mean = df_mean.set_index(\"Source\").loc[list(adata.var_names)]\n",
    "\n",
    "    return np.array(df_mean[\"Value\"])\n",
    "\n",
    "\n",
    "def run_DCI(adata, output_dir):\n",
    "    from causaldag import dci\n",
    "    from collections import Counter\n",
    "    import scipy\n",
    "\n",
    "    adata = adata.copy()\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "    X1 = adata.X[adata.obs[\"labels\"] == 0].astype(float)\n",
    "    X2 = adata.X[adata.obs[\"labels\"] == 1].astype(float)\n",
    "    X1 += np.random.normal(0, 1e-6, size=X1.shape)\n",
    "    X2 += np.random.normal(0, 1e-6, size=X2.shape)\n",
    "    p = X1.shape[1]\n",
    "    if scipy.sparse.issparse(adata.X):\n",
    "        X_full = adata.X.toarray()\n",
    "    else:\n",
    "        X_full = adata.X\n",
    "    corr = np.corrcoef(X_full.T)\n",
    "    threshold = 0.3  # 可调\n",
    "    candidate_edges = [(i, j) for i in range(p) for j in range(i + 1, p) if abs(corr[i, j]) >= threshold]\n",
    "    print(f\"[INFO] Filtered candidate edges: {len(candidate_edges)}\")\n",
    "    difference_matrix = dci(\n",
    "        X1,\n",
    "        X2,\n",
    "        difference_ug_method=\"constraint\",\n",
    "        difference_ug=candidate_edges,\n",
    "        alpha_ug=0.05,\n",
    "        alpha_skeleton=0.1,\n",
    "        max_set_size=2,\n",
    "    )\n",
    "    ddag_edges = set(zip(*np.where(difference_matrix != 0)))\n",
    "    print(\"len(ddag_edges)\", len(ddag_edges))\n",
    "    count_dict = Counter([node for edge in ddag_edges for node in edge])\n",
    "    count_df = pd.DataFrame(count_dict.items(), columns=[\"node\", \"count\"]).sort_values(\"count\", ascending=False)\n",
    "    full_df = pd.DataFrame({\"node\": adata.var_names})  # 创建完整的node列表\n",
    "    count_df[\"node\"] = count_df[\"node\"].map(full_df[\"node\"])\n",
    "    count_df = full_df.merge(count_df, on=\"node\", how=\"left\").fillna({\"count\": 0})\n",
    "    arr = np.array(count_df[\"count\"].values).reshape(1, -1)\n",
    "    normalized_arr = sk_normalize(arr, norm=\"l1\", axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_GENIE3(adata, output_dir):\n",
    "    from GENIE3 import GENIE3, get_link_list\n",
    "    import tempfile\n",
    "    import os\n",
    "\n",
    "    adata = adata.copy()\n",
    "    sc.pp.log1p(adata)\n",
    "    adata1 = adata[adata.obs[\"labels\"] == 1]\n",
    "    adata0 = adata[adata.obs[\"labels\"] == 0]\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp_file:\n",
    "        tmp_path = tmp_file.name\n",
    "    X = adata0.X\n",
    "    gene_names = list(adata.var_names)\n",
    "    VIM = GENIE3(X, gene_names=gene_names)\n",
    "    get_link_list(VIM, gene_names=gene_names, file_name=tmp_path)\n",
    "    df0 = pd.read_csv(tmp_path, sep=\"\\t\", header=None)\n",
    "    os.remove(tmp_path)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp_file:\n",
    "        tmp_path = tmp_file.name\n",
    "    X = adata1.X\n",
    "    gene_names = list(adata.var_names)\n",
    "    VIM = GENIE3(X, gene_names=gene_names)\n",
    "    get_link_list(VIM, gene_names=gene_names, file_name=tmp_path)\n",
    "    df1 = pd.read_csv(tmp_path, sep=\"\\t\", header=None)\n",
    "    os.remove(tmp_path)\n",
    "\n",
    "    merged = pd.merge(df1, df0, on=[0, 1], suffixes=(\"_net1\", \"_net0\"))\n",
    "    merged[\"Diff_Strength\"] = abs(merged[\"2_net1\"] - merged[\"2_net0\"])\n",
    "    diff_network = merged[[0, 1, \"Diff_Strength\"]]\n",
    "    arr = diff_network.groupby(0)[\"Diff_Strength\"].mean()\n",
    "    arr_sorted = arr.reindex(adata.var_names)\n",
    "    normalized_arr = minmax_scale(arr_sorted)\n",
    "\n",
    "    return normalized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd0fd632584cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_single_feature(score_array, threshold=None, topk=None):\n",
    "    \"\"\"\n",
    "    Select binary labels (1 for selected, 0 for not) from a 1D score array,\n",
    "    using either a threshold or top-k strategy.\n",
    "    \"\"\"\n",
    "    score_array = np.asarray(score_array)\n",
    "    n = len(score_array)\n",
    "\n",
    "    if topk is not None:\n",
    "        topk_indices = np.argsort(score_array)[-topk:]\n",
    "        selected = np.zeros(n, dtype=int)\n",
    "        selected[topk_indices] = 1\n",
    "        return selected\n",
    "\n",
    "    elif threshold is not None:\n",
    "        return (score_array >= threshold).astype(int)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Either threshold or topk must be specified.\")\n",
    "\n",
    "\n",
    "def calculate_metrics(weight_df, label_col=\"is_causal\", score_col=\"weight\", threshold=None, topk=None):\n",
    "    from sklearn.metrics import (\n",
    "        roc_curve,\n",
    "        auc,\n",
    "        confusion_matrix,\n",
    "        accuracy_score,\n",
    "        matthews_corrcoef,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        precision_recall_curve,\n",
    "    )\n",
    "\n",
    "    true_label = weight_df[label_col].values\n",
    "    score_array = weight_df[score_col].values\n",
    "\n",
    "    # Get predicted label by top-k or threshold\n",
    "    pred_label = select_single_feature(score_array, threshold=threshold, topk=topk)\n",
    "\n",
    "    # AUROC\n",
    "    fpr, tpr, _ = roc_curve(true_label, score_array)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # AUPR\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(true_label, score_array)\n",
    "    aupr = auc(recall_curve, precision_curve)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_label, pred_label)\n",
    "    TN, FP = cm[0, 0], cm[0, 1]\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "    # Other metrics\n",
    "    acc = accuracy_score(true_label, pred_label)\n",
    "    mcc = matthews_corrcoef(true_label, pred_label)\n",
    "    precision = precision_score(true_label, pred_label, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(true_label, pred_label, pos_label=1, zero_division=0)\n",
    "    f1 = f1_score(true_label, pred_label, pos_label=1, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"AUROC\": roc_auc,\n",
    "        \"AUPR\": aupr,\n",
    "        \"F1\": f1,\n",
    "        \"ACC\": acc,\n",
    "        \"MCC\": mcc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_prediction(true_label, pred_dict, topk=10):\n",
    "    \"\"\"\n",
    "    Evaluate each prediction array in pred_dict using standard metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for submethod, pred in pred_dict.items():\n",
    "        metrics = calculate_metrics(true_label, pred, topk=topk)\n",
    "        results.append(\n",
    "            {\n",
    "                \"SubMethod\": submethod,\n",
    "                \"AUROC\": metrics[0],\n",
    "                \"AUPR\": metrics[1],\n",
    "                \"F1\": metrics[2],\n",
    "                \"ACC\": metrics[3],\n",
    "                \"MCC\": metrics[4],\n",
    "                \"Precision\": metrics[5],\n",
    "                \"Recall\": metrics[6],\n",
    "                \"Specificity\": metrics[7],\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_layerwise_metrics(df, output_dir, causal_strength=0.4, p_zero=0.2):\n",
    "    \"\"\"\n",
    "    Generate Boxplot, Violinplot, and Barplot (mean±std) for each metric (AUROC, AUPR)\n",
    "    with solid box color, improved clarity, and Nature Methods-compatible visuals.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    sns.set_theme(style=\"white\")\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"font.size\": 14,\n",
    "            \"axes.labelsize\": 16,\n",
    "            \"axes.titlesize\": 18,\n",
    "            \"xtick.labelsize\": 12,\n",
    "            \"ytick.labelsize\": 12,\n",
    "            \"legend.fontsize\": 12,\n",
    "            \"figure.dpi\": 300,\n",
    "            \"savefig.dpi\": 300,\n",
    "            \"pdf.fonttype\": 42,\n",
    "            \"ps.fonttype\": 42,\n",
    "            \"font.family\": \"Arial\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Method\"] = df[\"Method\"].replace({\"CauTrigger_SHAP\": \"CauTrigger\", \"VAEgrad\": \"VAE\"})\n",
    "    method_order = [\"CauTrigger\", \"GENIE3\", \"SCRIBE\", \"PC\", \"VAE\", \"DCI\", \"MI\", \"RF\", \"SVM\"]\n",
    "    df[\"Method\"] = pd.Categorical(df[\"Method\"], categories=method_order, ordered=True)\n",
    "    method_count = len(method_order)\n",
    "\n",
    "    layer_levels = sorted(df[\"Layer\"].dropna().unique().tolist())\n",
    "    df[\"Layer\"] = pd.Categorical(df[\"Layer\"], categories=layer_levels, ordered=True)\n",
    "\n",
    "    layer_palette = {\n",
    "        \"layer1\": \"#3E4A89\",  # 深蓝紫 → 下游/靠近表型\n",
    "        \"layer2\": \"#74A9CF\",  # 浅蓝 → 上游/调控\n",
    "        \"layer3\": \"#D9EF8B\",  # 嫩黄绿 → 最上游因子\n",
    "        \"all\": \"#5B9BD5\",  # 🔁 改成清爽现代蓝\n",
    "    }\n",
    "\n",
    "    metrics = [\"AUROC\", \"AUPR\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        cs_tag = f\"cs{int(causal_strength * 100)}\"\n",
    "        p_tag = f\"p{p_zero}\"\n",
    "        metric_tag = metric.lower()\n",
    "\n",
    "        # === Boxplot (Optimized) ===\n",
    "        plt.figure(figsize=(7, 3.5))\n",
    "        ax = sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"Method\",\n",
    "            y=metric,\n",
    "            hue=\"Layer\",\n",
    "            palette=layer_palette,\n",
    "            width=0.6,\n",
    "            fliersize=0,\n",
    "            linewidth=0.6,\n",
    "            gap=0.1,  # hue box 之间留空\n",
    "            # boxprops=dict(edgecolor='#666666', linewidth=0.7),\n",
    "            # whiskerprops=dict(color='#999999', linewidth=0.6),\n",
    "            # capprops=dict(color='#999999', linewidth=0.6),\n",
    "            # medianprops=dict(color='black', linewidth=1.2)\n",
    "        )\n",
    "\n",
    "        # # 半透明填充颜色，柔和视觉\n",
    "        # for patch, (_, group) in zip(ax.patches, df.groupby([\"Method\", \"Layer\"])):\n",
    "        #     layer = group[\"Layer\"].iloc[0]\n",
    "        #     face_color = layer_palette.get(layer, \"#dddddd\")\n",
    "        #     patch.set_facecolor(face_color)\n",
    "        #     patch.set_alpha(0.9)\n",
    "\n",
    "        # 替代黑点为灰点，更柔和\n",
    "        sns.stripplot(\n",
    "            data=df,\n",
    "            x=\"Method\",\n",
    "            y=metric,\n",
    "            hue=\"Layer\",\n",
    "            dodge=True,\n",
    "            color=\"gray\",\n",
    "            size=2,\n",
    "            jitter=0.25,\n",
    "            alpha=0.3,\n",
    "            edgecolor=None,\n",
    "            linewidth=0,\n",
    "            legend=False,\n",
    "        )\n",
    "\n",
    "        # 分隔线更浅更细\n",
    "        for i in range(1, method_count):\n",
    "            ax.axvline(i - 0.5, linestyle=\"--\", color=\"lightgray\", linewidth=0.3, zorder=0)\n",
    "\n",
    "        # Legend\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        unique_layers = list(dict.fromkeys(zip(labels, handles)))\n",
    "        labels, handles = zip(*unique_layers)\n",
    "        plt.legend(\n",
    "            handles,\n",
    "            labels,\n",
    "            title=\"Layer\",\n",
    "            frameon=False,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 1.18),\n",
    "            ncol=len(labels),\n",
    "            fontsize=11,\n",
    "            title_fontsize=11,\n",
    "            columnspacing=1.2,\n",
    "            handlelength=1.5,\n",
    "        )\n",
    "\n",
    "        plt.ylabel(metric, fontsize=13)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=30, ha=\"right\", fontsize=11)\n",
    "        plt.yticks(fontsize=11)\n",
    "        plt.ylim(0, 1.05)\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        fname = f\"{metric_tag}-boxplot-{cs_tag}-{p_tag}\"\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.pdf\"))\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # === Violinplot ===\n",
    "        plt.figure(figsize=(7, 3.5))\n",
    "        ax = sns.violinplot(\n",
    "            data=df,\n",
    "            x=\"Method\",\n",
    "            y=metric,\n",
    "            hue=\"Layer\",\n",
    "            palette=layer_palette,\n",
    "            inner=\"quartile\",\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            bw=0.4,\n",
    "            width=0.7,\n",
    "            linewidth=1.0,\n",
    "            dodge=True,\n",
    "            saturation=0.8,\n",
    "        )\n",
    "        for i in range(1, method_count):\n",
    "            ax.axvline(i - 0.5, linestyle=\"--\", color=\"lightgray\", linewidth=0.5, zorder=0)\n",
    "        plt.ylabel(metric, fontsize=13)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=30, ha=\"right\", fontsize=11)\n",
    "        plt.yticks(fontsize=11)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend(\n",
    "            title=\"Layer\",\n",
    "            frameon=False,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 1.18),\n",
    "            ncol=len(set(df[\"Layer\"])),\n",
    "            fontsize=11,\n",
    "            title_fontsize=11,\n",
    "            columnspacing=1.2,\n",
    "            handlelength=1.5,\n",
    "        )\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        fname = f\"{metric_tag}-violinplot-{cs_tag}-{p_tag}\"\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.pdf\"))\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # === Barplot ===\n",
    "        summary_df = df.groupby([\"Method\", \"Layer\"])[metric].agg([\"mean\", \"std\"]).reset_index()\n",
    "        plt.figure(figsize=(7, 3.5))\n",
    "        ax = sns.barplot(\n",
    "            data=summary_df, x=\"Method\", y=\"mean\", hue=\"Layer\", palette=layer_palette, errorbar=None, width=0.7\n",
    "        )\n",
    "        for i in range(1, method_count):\n",
    "            ax.axvline(i - 0.5, linestyle=\"--\", color=\"lightgray\", linewidth=0.5, zorder=0)\n",
    "        patches = ax.patches\n",
    "        for patch, (_, row) in zip(patches, summary_df.iterrows()):\n",
    "            x = patch.get_x() + patch.get_width() / 2\n",
    "            y = patch.get_height()\n",
    "            yerr = row[\"std\"]\n",
    "            ax.errorbar(x=x, y=y, yerr=yerr, fmt=\"none\", ecolor=\"black\", elinewidth=1.5, capsize=4, capthick=1.5)\n",
    "        max_y = summary_df[\"mean\"].max() + summary_df[\"std\"].max() + 0.05\n",
    "        plt.ylim(0, max(1.05, max_y))\n",
    "        plt.ylabel(f\"{metric} (Mean ± SD)\", fontsize=13)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=30, ha=\"right\", fontsize=11)\n",
    "        plt.yticks(fontsize=11)\n",
    "        plt.legend(\n",
    "            title=\"Layer\",\n",
    "            frameon=False,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 1.18),\n",
    "            ncol=len(set(df[\"Layer\"])),\n",
    "            fontsize=11,\n",
    "            title_fontsize=11,\n",
    "            columnspacing=1.2,\n",
    "            handlelength=1.5,\n",
    "        )\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        fname = f\"{metric_tag}-barplot-{cs_tag}-{p_tag}\"\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.pdf\"))\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_aggregate_layerwise_metrics(\n",
    "    root_output_dir,\n",
    "    causal_strength_list,\n",
    "    p_zero_list,\n",
    "    spurious_mode=\"semi_hrc\",\n",
    "    n_hidden=10,\n",
    "    activation=\"linear\",\n",
    "    simulate_single_cell=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    汇总指定参数组合下的 Layerwise_Benchmark_Metrics.csv，并生成 AUROC / AUPR 的 3x3 boxplot 总图。\n",
    "    图例移至标题下方，适配 Nature Methods 风格。\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    sns.set_theme(style=\"white\")\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"font.size\": 12,\n",
    "            \"axes.labelsize\": 14,\n",
    "            \"axes.titlesize\": 16,\n",
    "            \"xtick.labelsize\": 12,\n",
    "            \"ytick.labelsize\": 12,\n",
    "            \"legend.fontsize\": 14,\n",
    "            \"figure.dpi\": 300,\n",
    "            \"savefig.dpi\": 300,\n",
    "            \"pdf.fonttype\": 42,\n",
    "            \"ps.fonttype\": 42,\n",
    "            \"font.family\": \"Arial\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for cs in causal_strength_list:\n",
    "        for pz in p_zero_list:\n",
    "            case_name = \"_\".join(\n",
    "                [\n",
    "                    \"2L_counts\",\n",
    "                    spurious_mode,\n",
    "                    f\"hidden{n_hidden}\",\n",
    "                    activation,\n",
    "                    f\"cs{int(cs * 100):02d}\",\n",
    "                    f\"p{pz}\",\n",
    "                    \"sc\" if simulate_single_cell else \"bulk\",\n",
    "                ]\n",
    "            )\n",
    "            csv_path = os.path.join(root_output_dir, case_name, \"Layerwise_Benchmark_Metrics.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df[\"ParamCombo\"] = f\"Causal Strength = {cs}, Sparsity = {pz}\"\n",
    "                all_dfs.append(df)\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"[WARN] No matching benchmark files found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    df[\"Method\"] = df[\"Method\"].replace({\"CauTrigger_SHAP\": \"CauTrigger\", \"VAEgrad\": \"VAE\"})\n",
    "    df[\"Method\"] = pd.Categorical(\n",
    "        df[\"Method\"], categories=[\"CauTrigger\", \"GENIE3\", \"SCRIBE\", \"PC\", \"VAE\", \"DCI\", \"MI\", \"RF\", \"SVM\"], ordered=True\n",
    "    )\n",
    "\n",
    "    tag_parts = [\n",
    "        \"2L_counts\",\n",
    "        spurious_mode,\n",
    "        f\"hidden{n_hidden}\",\n",
    "        activation,\n",
    "        \"sc\" if simulate_single_cell else \"bulk\",\n",
    "    ]\n",
    "    base_tag = \"_\".join(tag_parts)\n",
    "\n",
    "    for metric in [\"AUROC\", \"AUPR\"]:\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 12), sharey=True)\n",
    "        param_combos = sorted(df[\"ParamCombo\"].unique())\n",
    "\n",
    "        for ax, combo in zip(axes.flatten(), param_combos):\n",
    "            subdf = df[df[\"ParamCombo\"] == combo]\n",
    "            sns.boxplot(\n",
    "                data=subdf,\n",
    "                x=\"Method\",\n",
    "                y=metric,\n",
    "                hue=\"Layer\",\n",
    "                palette={\"layer1\": \"#3E4A89\", \"layer2\": \"#74A9CF\"},\n",
    "                ax=ax,\n",
    "                width=0.6,\n",
    "                fliersize=0,\n",
    "                linewidth=1,\n",
    "            )\n",
    "            ax.set_title(combo, fontsize=14)\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.tick_params(axis=\"x\", rotation=30)\n",
    "            ax.legend_.remove()\n",
    "            ax.grid(False)\n",
    "\n",
    "            n_methods = df[\"Method\"].nunique()\n",
    "            for i in range(1, n_methods):\n",
    "                ax.axvline(x=i - 0.5, linestyle=\"--\", color=\"lightgray\", linewidth=0.6, zorder=0)\n",
    "\n",
    "        # 设置主标题，并调整 y 值（不再为 legend 腾出空间）\n",
    "        fig.suptitle(f\"Layer-wise {metric} across methods\", fontsize=18, y=1.02)\n",
    "\n",
    "        # 添加统一 legend，放在标题正下方\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.5, 1.0), ncol=2, frameon=False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        out_prefix = f\"{base_tag}_boxplot_{metric}\"\n",
    "        fig.savefig(os.path.join(root_output_dir, f\"{out_prefix}.pdf\"), bbox_inches=\"tight\")\n",
    "        fig.savefig(os.path.join(root_output_dir, f\"{out_prefix}.png\"), bbox_inches=\"tight\")\n",
    "        print(f\"[INFO] Saved: {out_prefix}.pdf/.png\")\n",
    "\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e432f4d0e082cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(\n",
    "    algorithms, data_dir, output_dir, n_datasets=3, seed_list=None, save_adata=True, rerun=False, **generation_args\n",
    "):\n",
    "    \"\"\"\n",
    "    Run benchmark evaluation for causal discovery methods on synthetic two-layer datasets.\n",
    "    Supports both single-layer methods and recursive multi-layer methods like CauTrigger.\n",
    "    \"\"\"\n",
    "    algorithm_functions = {\n",
    "        # CauTrigger scoring variants\n",
    "        \"CauTrigger_Model\": lambda adata, out: run_CauTrigger(adata, out, mode=\"Model\"),\n",
    "        \"CauTrigger_Grad\": lambda adata, out: run_CauTrigger(adata, out, mode=\"Grad\"),\n",
    "        \"CauTrigger_SHAP\": lambda adata, out: run_CauTrigger(adata, out, mode=\"SHAP\"),\n",
    "        \"CauTrigger_Ensemble\": lambda adata, out: run_CauTrigger(adata, out, mode=\"Ensemble\"),\n",
    "        # Other baseline methods\n",
    "        \"PC\": run_PC,\n",
    "        \"VAEgrad\": run_VAE,\n",
    "        \"SVM\": run_SVM,\n",
    "        \"RF\": run_RF,\n",
    "        \"MI\": run_MI,\n",
    "        \"DCI\": run_DCI,\n",
    "        # \"NLBayes\": run_NLBAYES,\n",
    "        \"GENIE3\": run_GENIE3,\n",
    "        # \"GRNBOOST2\": run_GRNBOOST2,\n",
    "        \"SCRIBE\": run_SCRIBE,\n",
    "        \"VELORAMA\": run_VELORAMA,\n",
    "    }\n",
    "\n",
    "    print(f\"[INFO] Running benchmark on {n_datasets} datasets with algorithms: {algorithms}\")\n",
    "\n",
    "    if seed_list is None:\n",
    "        seed_list = list(range(n_datasets))\n",
    "    else:\n",
    "        if len(seed_list) < n_datasets:\n",
    "            max_seed = max(seed_list)\n",
    "            seed_list += list(range(max_seed + 1, max_seed + 1 + (n_datasets - len(seed_list))))\n",
    "        elif len(seed_list) > n_datasets:\n",
    "            seed_list = seed_list[:n_datasets]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, seed in enumerate(seed_list):\n",
    "        set_seed(seed)\n",
    "        adata = generate_two_layer_synthetic_data(seed=seed, **generation_args)\n",
    "        print(f\"[INFO] Dataset {i + 1}: Generated with seed {seed}\")\n",
    "\n",
    "        if save_adata:\n",
    "            dataset_dir = os.path.join(data_dir, f\"dataset{i + 1}\")\n",
    "            os.makedirs(dataset_dir, exist_ok=True)\n",
    "            adata.write(os.path.join(dataset_dir, \"adata.h5ad\"))\n",
    "\n",
    "        for algo in algorithms:\n",
    "            assert algo in algorithm_functions, f\"[ERROR] Algorithm '{algo}' not registered in algorithm_functions!\"\n",
    "\n",
    "            if algo.startswith(\"CauTrigger\"):\n",
    "                # --- Check if both layer1 and layer2 results exist ---\n",
    "                exist_all = True\n",
    "                for layer_name in [\"layer1\", \"layer2\"]:\n",
    "                    weight_path = os.path.join(output_dir, f\"weights_dataset{i}_{layer_name}_{algo}.csv\")\n",
    "                    if not os.path.exists(weight_path):\n",
    "                        exist_all = False\n",
    "                        break\n",
    "\n",
    "                if not rerun and exist_all:\n",
    "                    print(f\"[INFO] {algo} on dataset {i + 1} already exists. Loading...\")\n",
    "                    for layer_name in [\"layer1\", \"layer2\"]:\n",
    "                        weight_path = os.path.join(output_dir, f\"weights_dataset{i}_{layer_name}_{algo}.csv\")\n",
    "                        df = pd.read_csv(weight_path, index_col=0)\n",
    "                        metrics = calculate_metrics(df, score_col=\"weight\", label_col=\"is_causal\", topk=20)\n",
    "                        row = {\n",
    "                            \"Method\": algo,\n",
    "                            \"Layer\": layer_name,\n",
    "                            \"Dataset\": i,\n",
    "                            \"Seed\": seed,\n",
    "                            \"ScoreType\": \"weight\",\n",
    "                            **metrics,\n",
    "                        }\n",
    "                        all_results.append(row)\n",
    "                    continue\n",
    "\n",
    "                # --- If not exist or rerun, re-run ---\n",
    "                print(f\"[INFO] Evaluating {algo} on recursive 2-layer setting (Dataset {i + 1})...\")\n",
    "                func = algorithm_functions[algo]\n",
    "                pred_dict = func(adata, output_dir)\n",
    "\n",
    "                for layer_name, df in pred_dict.items():\n",
    "                    assert \"is_causal\" in df.columns, (\n",
    "                        f\"[ERROR] 'is_causal' column missing in CauTrigger output for {layer_name}\"\n",
    "                    )\n",
    "                    weight_path = os.path.join(output_dir, f\"weights_dataset{i}_{layer_name}_{algo}.csv\")\n",
    "                    df.to_csv(weight_path)\n",
    "                    print(f\"[INFO] Saved weights to: {weight_path}\")\n",
    "\n",
    "                    metrics = calculate_metrics(df, score_col=\"weight\", label_col=\"is_causal\", topk=20)\n",
    "                    row = {\n",
    "                        \"Method\": algo,\n",
    "                        \"Layer\": layer_name,\n",
    "                        \"Dataset\": i,\n",
    "                        \"Seed\": seed,\n",
    "                        \"ScoreType\": \"weight\",\n",
    "                        **metrics,\n",
    "                    }\n",
    "                    all_results.append(row)\n",
    "\n",
    "            else:\n",
    "                for layer_name in adata.var[\"layer\"].unique():\n",
    "                    layer_vars = adata.var_names[adata.var[\"layer\"] == layer_name]\n",
    "                    sub_adata = adata[:, layer_vars]\n",
    "\n",
    "                    weight_path = os.path.join(output_dir, f\"weights_dataset{i}_{layer_name}_{algo}.csv\")\n",
    "                    # if algo != \"DCI\" and not rerun and os.path.exists(weight_path):\n",
    "                    if not rerun and os.path.exists(weight_path):\n",
    "                        print(f\"[INFO] {algo} on {layer_name} (Dataset {i + 1}) already exists. Loading...\")\n",
    "                        weight_df = pd.read_csv(weight_path, index_col=0)\n",
    "                        metrics = calculate_metrics(weight_df, score_col=\"weight\", label_col=\"is_causal\", topk=20)\n",
    "                        row = {\n",
    "                            \"Method\": algo,\n",
    "                            \"Layer\": layer_name,\n",
    "                            \"Dataset\": i,\n",
    "                            \"Seed\": seed,\n",
    "                            \"ScoreType\": \"weight\",\n",
    "                            **metrics,\n",
    "                        }\n",
    "                        all_results.append(row)\n",
    "                        continue\n",
    "\n",
    "                    print(f\"[INFO] Evaluating {algo} on {layer_name} (Dataset {i + 1})...\")\n",
    "                    func = algorithm_functions[algo]\n",
    "                    pred = func(sub_adata, output_dir)\n",
    "\n",
    "                    pred_dict = {\"weight\": pred} if not isinstance(pred, dict) else pred\n",
    "                    weight_df = pd.DataFrame(pred_dict, index=layer_vars)\n",
    "                    weight_df[\"is_causal\"] = adata.var.loc[layer_vars, \"is_causal\"].values\n",
    "\n",
    "                    weight_df.to_csv(weight_path)\n",
    "                    print(f\"[INFO] Saved weights to: {weight_path}\")\n",
    "\n",
    "                    metrics = calculate_metrics(weight_df, score_col=\"weight\", label_col=\"is_causal\", topk=20)\n",
    "                    row = {\n",
    "                        \"Method\": algo,\n",
    "                        \"Layer\": layer_name,\n",
    "                        \"Dataset\": i,\n",
    "                        \"Seed\": seed,\n",
    "                        \"ScoreType\": \"weight\",\n",
    "                        **metrics,\n",
    "                    }\n",
    "                    all_results.append(row)\n",
    "\n",
    "        del adata\n",
    "        gc.collect()\n",
    "\n",
    "    # Save final metrics\n",
    "    df = pd.DataFrame(all_results)\n",
    "    metrics_path = os.path.join(output_dir, \"Layerwise_Benchmark_Metrics.csv\")\n",
    "    df.to_csv(metrics_path, index=False)\n",
    "    print(f\"[INFO] Saved final evaluation to: {metrics_path}\")\n",
    "\n",
    "    # Draw plots\n",
    "    plot_layerwise_metrics(\n",
    "        df,\n",
    "        output_dir,\n",
    "        causal_strength=generation_args.get(\"causal_strength\", 0.4),\n",
    "        p_zero=generation_args.get(\"p_zero\", 0.2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe44c0e188aee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/mnt/e/Project_Research/CauTrigger_Project/CauTrigger-master\"\n",
    "case_dir = os.path.join(BASE_DIR, \"simulation\")\n",
    "\n",
    "# ==== 固定参数 ====\n",
    "n_datasets = 10\n",
    "spurious_mode = \"semi_hrc\"\n",
    "n_hidden = 10\n",
    "activation = \"linear\"\n",
    "simulate_single_cell = True\n",
    "\n",
    "# algorithms = ['CauTrigger_SHAP', 'SVM', 'RF', 'MI']\n",
    "algorithms = [\"CauTrigger_SHAP\", \"GENIE3\", \"SCRIBE\", \"PC\", \"VAEgrad\", \"DCI\", \"MI\", \"RF\", \"SVM\"]\n",
    "# algorithms = ['DCI',]\n",
    "\n",
    "# ==== 你想扫的参数范围 ====\n",
    "causal_strength_list = [0.3, 0.4, 0.5, 0.6, 0.7]  # 因果强度：低、中、高\n",
    "p_zero_list = [0.1, 0.3, 0.5, 0.7]  # 稀疏度：低、中、高\n",
    "causal_strength_list = [0.3, 0.4, 0.5]  # 因果强度：低、中、高\n",
    "p_zero_list = [0.3, 0.5, 0.7]  # 稀疏度：低、中、高\n",
    "\n",
    "for causal_strength in causal_strength_list:\n",
    "    for p_zero in p_zero_list:\n",
    "        # ==== 动态生成一组参数 ====\n",
    "        generation_args = dict(\n",
    "            spurious_mode=spurious_mode,\n",
    "            n_hidden=n_hidden,\n",
    "            activation=activation,\n",
    "            causal_strength=causal_strength,\n",
    "            p_zero=p_zero,\n",
    "            simulate_single_cell=simulate_single_cell,\n",
    "        )\n",
    "\n",
    "        # ==== 动态生成 case_name ====\n",
    "        tag_parts = [\n",
    "            \"2L_counts\",\n",
    "            spurious_mode,\n",
    "            f\"hidden{n_hidden}\",\n",
    "            activation,\n",
    "            f\"cs{int(causal_strength * 100):02d}\",\n",
    "            f\"p{p_zero}\",\n",
    "            \"sc\" if simulate_single_cell else \"bulk\",\n",
    "        ]\n",
    "        case_name = \"_\".join(tag_parts)\n",
    "\n",
    "        data_dir = os.path.join(case_dir, \"data\", case_name)\n",
    "        output_dir = os.path.join(case_dir, \"output\", case_name)\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"=== Running: {case_name} ===\")\n",
    "\n",
    "        run_benchmark(\n",
    "            algorithms=algorithms, data_dir=data_dir, output_dir=output_dir, n_datasets=n_datasets, **generation_args\n",
    "        )\n",
    "\n",
    "# 最后生成汇总图（只处理当前这批组合）\n",
    "aggregate_output_root = os.path.join(case_dir, \"output\")\n",
    "plot_aggregate_layerwise_metrics(\n",
    "    root_output_dir=os.path.join(case_dir, \"output\"),\n",
    "    causal_strength_list=causal_strength_list,\n",
    "    p_zero_list=p_zero_list,\n",
    "    spurious_mode=spurious_mode,\n",
    "    n_hidden=n_hidden,\n",
    "    activation=activation,\n",
    "    simulate_single_cell=simulate_single_cell,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafdc6afd4d220c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f94f2c624509f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ee3a3b2c9c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
